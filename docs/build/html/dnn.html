<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>dnn package &mdash; RepLearn 1.0 documentation</title>
    
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <link rel="top" title="RepLearn 1.0 documentation" href="index.html" />
   
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9">

  </head>
  <body role="document">  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="dnn-package">
<h1>dnn package<a class="headerlink" href="#dnn-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-dnn.activation">
<span id="dnn-activation-module"></span><h2>dnn.activation module<a class="headerlink" href="#module-dnn.activation" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="dnn.activation.leaky_relu">
<code class="descclassname">dnn.activation.</code><code class="descname">leaky_relu</code><span class="sig-paren">(</span><em>features</em>, <em>alpha=0.01</em><span class="sig-paren">)</span><a class="headerlink" href="#dnn.activation.leaky_relu" title="Permalink to this definition">¶</a></dt>
<dd><p>Leaky RELU - Prevents dying neurons due to gradient explosion
:param features: The input to the RELU layer
:param alpha: The slope of the negative input values
:return: relu activation output</p>
</dd></dl>

<dl class="function">
<dt id="dnn.activation.relu">
<code class="descclassname">dnn.activation.</code><code class="descname">relu</code><span class="sig-paren">(</span><em>features</em><span class="sig-paren">)</span><a class="headerlink" href="#dnn.activation.relu" title="Permalink to this definition">¶</a></dt>
<dd><p>RELU activations
:param features: The input to the RELU layer
:return: relu activation output</p>
</dd></dl>

</div>
<div class="section" id="module-dnn.layers">
<span id="dnn-layers-module"></span><h2>dnn.layers module<a class="headerlink" href="#module-dnn.layers" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="dnn.layers.Layers">
<em class="property">class </em><code class="descclassname">dnn.layers.</code><code class="descname">Layers</code><a class="headerlink" href="#dnn.layers.Layers" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><code class="xref py py-class docutils literal"><span class="pre">object</span></code></a></p>
<dl class="method">
<dt id="dnn.layers.Layers.add_activation_layer">
<code class="descname">add_activation_layer</code><span class="sig-paren">(</span><em>input_layer_id</em>, <em>layer_name</em>, <em>activation_type='relu'</em><span class="sig-paren">)</span><a class="headerlink" href="#dnn.layers.Layers.add_activation_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds the activation layer
:param input_layer_id: The input layer identifier
:param layer_name: The name of the layer. Type=string
:param activation_type: &#8216;relu&#8217; for RELU and &#8216;leaky-relu&#8217; for Leaky RELU. Default = RELU
:return: None</p>
</dd></dl>

<dl class="method">
<dt id="dnn.layers.Layers.add_ground_truth_layer">
<code class="descname">add_ground_truth_layer</code><span class="sig-paren">(</span><em>width</em>, <em>layer_name='ground_truth'</em><span class="sig-paren">)</span><a class="headerlink" href="#dnn.layers.Layers.add_ground_truth_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds ground truth layer to the model
:param width: The width of the ground truth = dimension of the input
:param layer_name: The name of the layer. Type=string
:return: None</p>
</dd></dl>

<dl class="method">
<dt id="dnn.layers.Layers.add_hidden_layer">
<code class="descname">add_hidden_layer</code><span class="sig-paren">(</span><em>input_layer_id</em>, <em>input_width</em>, <em>output_width</em>, <em>layer_name</em>, <em>batch_norm=False</em>, <em>sharing=False</em><span class="sig-paren">)</span><a class="headerlink" href="#dnn.layers.Layers.add_hidden_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds the hidden layer to the model
:param input_layer_id: The input layer identifier
:param input_width: The width of the input for this layer
:param output_width: The width of the output for this layer
:param layer_name: The name of the layer. Type=string
:param batch_norm: True -&gt; if next layer is a batch normalization layer, else False. Default= False
:param sharing: True, if the layer is shared, else False.. Default=False
:return: None</p>
</dd></dl>

<dl class="method">
<dt id="dnn.layers.Layers.add_input_layer">
<code class="descname">add_input_layer</code><span class="sig-paren">(</span><em>width</em>, <em>layer_name='input'</em><span class="sig-paren">)</span><a class="headerlink" href="#dnn.layers.Layers.add_input_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds input layer to the model
:param width: The width of the input = dimension of the input
:param layer_name: The name of the layer. Type=string
:return: None</p>
</dd></dl>

<dl class="method">
<dt id="dnn.layers.Layers.add_loss_layer">
<code class="descname">add_loss_layer</code><span class="sig-paren">(</span><em>layer_name</em>, <em>prediction_layer_id</em>, <em>ground_truth_layer_id</em>, <em>loss_type</em><span class="sig-paren">)</span><a class="headerlink" href="#dnn.layers.Layers.add_loss_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds a layer corresponding to the loss function
:param layer_name: The name of the layer. Type=string
:param prediction_layer_id: The identifier for the prediction layer
:param ground_truth_layer_id: The identifier for the ground truth layer
:param loss_type: The loss function to use. Available options defined by LossTypes.
:return: None</p>
</dd></dl>

<dl class="method">
<dt id="dnn.layers.Layers.add_output_layer">
<code class="descname">add_output_layer</code><span class="sig-paren">(</span><em>input_layer_id</em>, <em>input_width</em>, <em>output_width</em>, <em>layer_name='output'</em><span class="sig-paren">)</span><a class="headerlink" href="#dnn.layers.Layers.add_output_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds the output layer to the model
:param input_layer_id: The input layer identifier
:param input_width: The width of the input for this layer
:param output_width: The width of the output for this layer
:param layer_name: The name of the layer. Type=string
:return: None</p>
</dd></dl>

<dl class="method">
<dt id="dnn.layers.Layers.add_regularization_layer">
<code class="descname">add_regularization_layer</code><span class="sig-paren">(</span><em>input_layer_id</em>, <em>layer_name</em>, <em>regularization_type='dropout'</em>, <em>epsilon=None</em>, <em>dropout_ratio=None</em><span class="sig-paren">)</span><a class="headerlink" href="#dnn.layers.Layers.add_regularization_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds the regularization layer to the model
:param input_layer_id: The input layer identifier
:param layer_name: The name of the layer. Type=string
:param regularization_type: &#8216;dropout&#8217; for Dropout and &#8216;batch_norm&#8217; for Batch Normalization. Default = &#8216;dropout&#8217;
:param epsilon: The batch_norm parameter to ensure that division is not by zero when variance of batch = 0
:param dropout_ratio: The fraction of the layers to be masked.
:return: None</p>
</dd></dl>

<dl class="method">
<dt id="dnn.layers.Layers.get_layer">
<code class="descname">get_layer</code><span class="sig-paren">(</span><em>layer_id</em><span class="sig-paren">)</span><a class="headerlink" href="#dnn.layers.Layers.get_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Retuns the tensorflow object corresponding to the requested layer
:param layer_id: Layer identifier
:return: Tensorflow layer object</p>
</dd></dl>

<dl class="method">
<dt id="dnn.layers.Layers.name_network">
<code class="descname">name_network</code><span class="sig-paren">(</span><em>name</em><span class="sig-paren">)</span><a class="headerlink" href="#dnn.layers.Layers.name_network" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="dnn.layers.Layers.network_type">
<code class="descname">network_type</code><span class="sig-paren">(</span><em>is_first</em><span class="sig-paren">)</span><a class="headerlink" href="#dnn.layers.Layers.network_type" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="module-dnn.loss">
<span id="dnn-loss-module"></span><h2>dnn.loss module<a class="headerlink" href="#module-dnn.loss" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="dnn.loss.cross_entropy">
<code class="descclassname">dnn.loss.</code><code class="descname">cross_entropy</code><span class="sig-paren">(</span><em>ground_truth</em>, <em>output</em><span class="sig-paren">)</span><a class="headerlink" href="#dnn.loss.cross_entropy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the Cross-Entropy loss container
:param ground_truth: The ground truth layer
:param output: The network output layer
:return: Cross-entropy error</p>
</dd></dl>

<dl class="function">
<dt id="dnn.loss.mse">
<code class="descclassname">dnn.loss.</code><code class="descname">mse</code><span class="sig-paren">(</span><em>ground_truth</em>, <em>output</em><span class="sig-paren">)</span><a class="headerlink" href="#dnn.loss.mse" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the Mean Squared Error loss container
:param ground_truth: The ground truth layer
:param output: The network output layer
:return: MSE error</p>
</dd></dl>

</div>
<div class="section" id="module-dnn.optimizer">
<span id="dnn-optimizer-module"></span><h2>dnn.optimizer module<a class="headerlink" href="#module-dnn.optimizer" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="dnn.optimizer.Optimizer">
<em class="property">class </em><code class="descclassname">dnn.optimizer.</code><code class="descname">Optimizer</code><span class="sig-paren">(</span><em>cost</em><span class="sig-paren">)</span><a class="headerlink" href="#dnn.optimizer.Optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/library/functions.html#object" title="(in Python v2.7)"><code class="xref py py-class docutils literal"><span class="pre">object</span></code></a></p>
<dl class="method">
<dt id="dnn.optimizer.Optimizer.get_ada_delta">
<code class="descname">get_ada_delta</code><span class="sig-paren">(</span><em>learning_rate</em><span class="sig-paren">)</span><a class="headerlink" href="#dnn.optimizer.Optimizer.get_ada_delta" title="Permalink to this definition">¶</a></dt>
<dd><p>Ada Delta Optimizer
:param learning_rate: The learning rate
:return: the optimizer</p>
</dd></dl>

<dl class="method">
<dt id="dnn.optimizer.Optimizer.get_adagrad">
<code class="descname">get_adagrad</code><span class="sig-paren">(</span><em>learning_rate</em><span class="sig-paren">)</span><a class="headerlink" href="#dnn.optimizer.Optimizer.get_adagrad" title="Permalink to this definition">¶</a></dt>
<dd><p>Adagrad optimizer
:param learning_rate: The learning rate
:return: the optimizer</p>
</dd></dl>

<dl class="method">
<dt id="dnn.optimizer.Optimizer.get_adam">
<code class="descname">get_adam</code><span class="sig-paren">(</span><em>learning_rate</em><span class="sig-paren">)</span><a class="headerlink" href="#dnn.optimizer.Optimizer.get_adam" title="Permalink to this definition">¶</a></dt>
<dd><p>Adam optimizer
:param learning_rate: The learning rate
:return: the optimizer</p>
</dd></dl>

<dl class="method">
<dt id="dnn.optimizer.Optimizer.get_momentum">
<code class="descname">get_momentum</code><span class="sig-paren">(</span><em>learning_rate</em>, <em>momentum</em><span class="sig-paren">)</span><a class="headerlink" href="#dnn.optimizer.Optimizer.get_momentum" title="Permalink to this definition">¶</a></dt>
<dd><p>Momentum optimizer
:param learning_rate: The learning rate
:param momentum: The momentum parameter for the optimizer
:return: the optimizer</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-dnn.parameters">
<span id="dnn-parameters-module"></span><h2>dnn.parameters module<a class="headerlink" href="#module-dnn.parameters" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="dnn.parameters.bias_variable">
<code class="descclassname">dnn.parameters.</code><code class="descname">bias_variable</code><span class="sig-paren">(</span><em>shape</em>, <em>scope=None</em><span class="sig-paren">)</span><a class="headerlink" href="#dnn.parameters.bias_variable" title="Permalink to this definition">¶</a></dt>
<dd><p>This creates the bias placeholder for the DNN layers
:param shape: shape of the bias variable
:param scope: scope of the bias variable
:return: The tensorflow placeholder for the bias</p>
</dd></dl>

<dl class="function">
<dt id="dnn.parameters.weight_variable">
<code class="descclassname">dnn.parameters.</code><code class="descname">weight_variable</code><span class="sig-paren">(</span><em>shape</em>, <em>scope=None</em><span class="sig-paren">)</span><a class="headerlink" href="#dnn.parameters.weight_variable" title="Permalink to this definition">¶</a></dt>
<dd><p>This creates the weight placeholder for the DNN layers
:param shape: shape of the weight variable
:param scope: scope of the weight variable
:return: The tensorflow placeholder for weights</p>
</dd></dl>

</div>
<div class="section" id="module-dnn.regularization">
<span id="dnn-regularization-module"></span><h2>dnn.regularization module<a class="headerlink" href="#module-dnn.regularization" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="dnn.regularization.batch_norm_layer">
<code class="descclassname">dnn.regularization.</code><code class="descname">batch_norm_layer</code><span class="sig-paren">(</span><em>input_layer</em>, <em>epsilon=1e-05</em><span class="sig-paren">)</span><a class="headerlink" href="#dnn.regularization.batch_norm_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>The batch normalization applied to the output of the layer before applying the activation.</p>
<p><strong>* Note - If applying this layer, make sure that previous hidden layer has batch_norm flag = True *</strong></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>input_layer</strong> &#8211; The input of the present layer, and the output of the previous layer</li>
<li><strong>epsilon</strong> &#8211; The small value added to the denominator to prevent division by 0, when the variance = 0</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">The batch normalization output. This output should be applied with the activation</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="dnn.regularization.dropout_layer">
<code class="descclassname">dnn.regularization.</code><code class="descname">dropout_layer</code><span class="sig-paren">(</span><em>input_layer</em>, <em>dropout_ratio=0.5</em><span class="sig-paren">)</span><a class="headerlink" href="#dnn.regularization.dropout_layer" title="Permalink to this definition">¶</a></dt>
<dd><p>Forms a dropout layer
:param input_layer: The layer on which the dropout is to be applied
:param dropout_ratio: The ratio of the dropout that is to be applied
:return: The dropout layer</p>
</dd></dl>

</div>
<div class="section" id="module-dnn">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-dnn" title="Permalink to this headline">¶</a></h2>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">dnn package</a><ul>
<li><a class="reference internal" href="#submodules">Submodules</a></li>
<li><a class="reference internal" href="#module-dnn.activation">dnn.activation module</a></li>
<li><a class="reference internal" href="#module-dnn.layers">dnn.layers module</a></li>
<li><a class="reference internal" href="#module-dnn.loss">dnn.loss module</a></li>
<li><a class="reference internal" href="#module-dnn.optimizer">dnn.optimizer module</a></li>
<li><a class="reference internal" href="#module-dnn.parameters">dnn.parameters module</a></li>
<li><a class="reference internal" href="#module-dnn.regularization">dnn.regularization module</a></li>
<li><a class="reference internal" href="#module-dnn">Module contents</a></li>
</ul>
</li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/dnn.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2017, Harshal Priyadarshi, Shobhit Chaurasia, Tyler McDonell.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.3.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.6</a>
      
      |
      <a href="_sources/dnn.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>